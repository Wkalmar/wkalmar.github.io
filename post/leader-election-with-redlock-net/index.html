<!doctype html>
<html lang="en-us">
  <head>
    <title>Distributed locking with Redlock.net // Bohdan Stupak&#39;s blog</title>
    <meta charset="utf-8" />
    <meta name="generator" content="Hugo 0.55.6" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="author" content="John Doe" />
    <meta name="description" content="" />
    <link rel="stylesheet" href="https://wkalmar.github.io/css/main.min.f90f5edd436ec7b74ad05479a05705770306911f721193e7845948fb07fe1335.css" />

    
    <meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Distributed locking with Redlock.net"/>
<meta name="twitter:description" content="Why locking things Microservice architecture becomes widely adopted these days. One of the benefits it offers is the possibility of horizontal scaling which allows us to increase the performance of our application dramatically. However, there are situations when multiple instances of service face contention for some shared resource. In such a case one of the instances would acquire a lock over this resource claiming exclusive access to it. Let&rsquo;s have a look at some possible use cases when this is helpful."/>

    <meta property="og:title" content="Distributed locking with Redlock.net" />
<meta property="og:description" content="Why locking things Microservice architecture becomes widely adopted these days. One of the benefits it offers is the possibility of horizontal scaling which allows us to increase the performance of our application dramatically. However, there are situations when multiple instances of service face contention for some shared resource. In such a case one of the instances would acquire a lock over this resource claiming exclusive access to it. Let&rsquo;s have a look at some possible use cases when this is helpful." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://wkalmar.github.io/post/leader-election-with-redlock-net/" />
<meta property="article:published_time" content="2019-07-19T00:00:00&#43;00:00"/>
<meta property="article:modified_time" content="2019-07-19T00:00:00&#43;00:00"/>


  </head>
  <body>
    <header class="app-header">
      <a href="https://wkalmar.github.io/"><img class="app-header-avatar" src="/avatar1.jpg" alt="John Doe" /></a>
      <h1>Bohdan Stupak&#39;s blog</h1>
      <p>Continious learner from Kyiv, Ukraine</p>
      <div class="app-header-social">
        
          <a target="_blank" href="https://github.com/Wkalmar" rel="noreferrer noopener"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="1" stroke-linecap="round" stroke-linejoin="round" class="icon icon-github">
  <title>github</title>
  <path d="M9 19c-5 1.5-5-2.5-7-3m14 6v-3.87a3.37 3.37 0 0 0-.94-2.61c3.14-.35 6.44-1.54 6.44-7A5.44 5.44 0 0 0 20 4.77 5.07 5.07 0 0 0 19.91 1S18.73.65 16 2.48a13.38 13.38 0 0 0-7 0C6.27.65 5.09 1 5.09 1A5.07 5.07 0 0 0 5 4.77a5.44 5.44 0 0 0-1.5 3.78c0 5.42 3.3 6.61 6.44 7A3.37 3.37 0 0 0 9 18.13V22"></path>
</svg></a>
        
          <a target="_blank" href="https://twitter.com/bohdanstupak1" rel="noreferrer noopener"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="1" stroke-linecap="round" stroke-linejoin="round" class="icon icon-twitter">
  <title>twitter</title>
  <path d="M23 3a10.9 10.9 0 0 1-3.14 1.53 4.48 4.48 0 0 0-7.86 3v1A10.66 10.66 0 0 1 3 4s-4 9 5 13a11.64 11.64 0 0 1-7 2c9 5 20 0 20-11.5a4.5 4.5 0 0 0-.08-.83A7.72 7.72 0 0 0 23 3z"></path>
</svg></a>
        
          <a target="_blank" href="https://stackoverflow.com/users/11306392/bohdan-stupak?tab=profile" rel="noreferrer noopener"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="1" stroke-linecap="round" stroke-linejoin="round" class="icon icon-stackoverflow">
  <title>stackoverflow</title>
  <path d="M18.986 21.865v-6.404h2.134V24H1.844v-8.539h2.13v6.404h15.012zM6.111 19.731H16.85v-2.137H6.111v2.137zm.259-4.852l10.48 2.189.451-2.07-10.478-2.187-.453 2.068zm1.359-5.056l9.705 4.53.903-1.95-9.706-4.53-.902 1.936v.014zm2.715-4.785l8.217 6.855 1.359-1.62-8.216-6.853-1.35 1.617-.01.001zM15.751 0l-1.746 1.294 6.405 8.604 1.746-1.294L15.749 0h.002z"/>
</svg></a>
        
          <a target="_blank" href="https://www.linkedin.com/in/bohdan-stupak-19232aaa/" rel="noreferrer noopener"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="1" stroke-linecap="round" stroke-linejoin="round" class="icon icon-linkedin">
  <title>linkedin</title>
  <path d="M16 8a6 6 0 0 1 6 6v7h-4v-7a2 2 0 0 0-2-2 2 2 0 0 0-2 2v7h-4v-7a6 6 0 0 1 6-6z"></path><rect x="2" y="9" width="4" height="12"></rect><circle cx="4" cy="4" r="2"></circle>
</svg></a>
        
      </div>
    </header>
    <main class="app-container">
      
  <article class="post">
    <header class="post-header">
      <h1 class ="post-title">Distributed locking with Redlock.net</h1>
      <div class="post-meta">
        <div>
          <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="1" stroke-linecap="round" stroke-linejoin="round" class="icon icon-calendar">
  <title>calendar</title>
  <rect x="3" y="4" width="18" height="18" rx="2" ry="2"></rect><line x1="16" y1="2" x2="16" y2="6"></line><line x1="8" y1="2" x2="8" y2="6"></line><line x1="3" y1="10" x2="21" y2="10"></line>
</svg>
          Jul 19, 2019
        </div>
        <div>
          <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="1" stroke-linecap="round" stroke-linejoin="round" class="icon icon-clock">
  <title>clock</title>
  <circle cx="12" cy="12" r="10"></circle><polyline points="12 6 12 12 16 14"></polyline>
</svg>
          10 min read
        </div><div>
          <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="1" stroke-linecap="round" stroke-linejoin="round" class="icon icon-tag">
  <title>tag</title>
  <path d="M20.59 13.41l-7.17 7.17a2 2 0 0 1-2.83 0L2 12V2h10l8.59 8.59a2 2 0 0 1 0 2.82z"></path><line x1="7" y1="7" x2="7" y2="7"></line>
</svg>
          <a class="tag" href="https://wkalmar.github.io/tags/csharp/">Csharp</a><a class="tag" href="https://wkalmar.github.io/tags/.net/">.NET</a><a class="tag" href="https://wkalmar.github.io/tags/redis/">Redis</a><a class="tag" href="https://wkalmar.github.io/tags/microservices/">Microservices</a><a class="tag" href="https://wkalmar.github.io/tags/architecture/">Architecture</a></div></div>
    </header>
    <div class="post-content">
      

<h2 id="why-locking-things">Why locking things</h2>

<p>Microservice architecture becomes widely adopted these days. One of the benefits it offers is the possibility of horizontal scaling which allows us to increase the performance of our application dramatically. However, there are situations when multiple instances of service face contention for some shared resource. In such a case one of the instances would acquire a lock over this resource claiming exclusive access to it. Let&rsquo;s have a look at some possible use cases when this is helpful.</p>

<h3 id="sending-notifications">Sending notifications</h3>

<p>Let&rsquo;s imagine a service that takes a batch of notifications from the database and sends them to customers. Service is designed as a background worker which operates once in a given period of time. In case we want to increase the performance of the service via horizontal scaling we need to somehow signal that the batch is consumed by a given instance of the service. We could create a flag inside the database and set it if the messages are occupied by worker instance. However, such a solution would pollute our data model. Instead, we can place a lock for a given batch inside a distributed storage.</p>

<h3 id="poor-man-s-failover">Poor man&rsquo;s failover</h3>

<p>Another reason might be poor-man&rsquo;s failover scenario, where one instance of a service executes work, while the other is idle and waits just in case the first instance fails for some reason.</p>

<h3 id="ensuring-exactly-once-message-processing">Ensuring exactly once message processing</h3>

<p>Imagine multiple service instances consuming messages from the stream. Depending on stream implementation, this may lead to every instance consuming the same message. One of the possible solutions is to shard stream creating a sub-stream for every dedicated instance. The disadvantage of this solution is that we have to reconfigure the stream each time we upscale/downscale service.
An alternative solution would be each service actually consume the message but try to acquire a distributed lock on this message before processing it. This way each service will be processed only once.</p>

<h2 id="using-redis">Using Redis</h2>

<p>As storage for a distributed lock, we’ll use Redis. Redis has a number of advantages:</p>

<ul>
<li>Redis is in-memory key-value storage that provides additional speed.</li>
<li>It has the ability to set TTL value for a key which allows us to expire lock. We will see later that this is the important part of the algotighm.</li>
</ul>

<p>There is already a <a href="https://github.com/samcook/RedLock.net">library</a> that implements distributed lock over Redis. So we just have to make use of it.</p>

<h3 id="redlock-implementation">Redlock implementation</h3>

<p>Let&rsquo;s imagine we&rsquo;ll use a single Redis as a storage lock. Such solution has an obvious downside: it is the single point of failure. However, you can&rsquo;t mitigate this issue by simply adding a replica because it will lead to a race condition. Imagine the following scenario:</p>

<ol>
<li>Client A acquires the lock in the master.</li>
<li>The master crashes before the write to the key is transmitted to the replica.</li>
<li>The replica gets promoted to master.</li>
<li>Client B acquires the lock to the same resource A already holds a lock for.</li>
</ol>

<p>So how do you implement the algorithm correctly?</p>

<p>In the distributed version of the algorithm we assume we have N Redis masters. Those nodes are totally independent, so we don’t use replication or any other implicit coordination system. We already described how to acquire and release the lock safely in a single instance. We take for granted that the algorithm will use this method to acquire and release the lock in a single instance. In our examples we set N=5, which is a reasonable value, so we need to run 5 Redis masters on different computers or virtual machines in order to ensure that they’ll fail in a mostly independent way.</p>

<p>In order to acquire the lock, the client performs the following operations:</p>

<ol>
<li>It gets the current time in milliseconds.</li>
<li>It tries to acquire the lock in all the N instances sequentially, using the same key name and random value in all the instances. During step 2, when setting the lock in each instance, the client uses a timeout which is small compared to the total lock auto-release time in order to acquire it. For example if the auto-release time is 10 seconds, the timeout could be in the ~ 5-50 milliseconds range. This prevents the client from remaining blocked for a long time trying to talk with a Redis node which is down: if an instance is not available, we should try to talk with the next instance ASAP.</li>
<li>The client computes how much time elapsed in order to acquire the lock, by subtracting from the current time the timestamp obtained in step 1. If and only if the client was able to acquire the lock in the majority of the instances (at least 3), and the total time elapsed to acquire the lock is less than lock validity time, the lock is considered to be acquired.</li>
<li>If the lock was acquired, its validity time is considered to be the initial validity time minus the time elapsed, as computed in step 3.</li>
<li>If the client failed to acquire the lock for some reason (either it was not able to lock <code>N/2+1</code> instances or the validity time is negative), it will try to unlock all the instances (even the instances it believed it was not able to lock).</li>
</ol>

<h3 id="is-redlock-safe">Is Redlock safe?</h3>

<p>In order to be considered safe Redlock algorithm must satify the following properties:</p>

<ol>
<li>Safety property: Mutual exclusion. At any given moment, only one client can hold a lock.</li>
<li>Liveness property A: Deadlock free. Eventually it is always possible to acquire a lock, even if the client that locked a resource crashes or gets partitioned.</li>
<li>Liveness property B: Fault tolerance. As long as the majority of Redis nodes are up, clients are able to acquire and release locks.</li>
</ol>

<p>Let&rsquo;s examine various scenarios to see if Redlock conforms to these properties.</p>

<p>To start let’s assume that a client is able to acquire the lock in the majority of instances. All the instances will contain a key with the same time to live. However, the key was set at different times, so the keys will also expire at different times. But if the first key was set at worst at time <code>T1</code> (the time we sample before contacting the first server) and the last key was set at worst at time <code>T2</code> (the time we obtained the reply from the last server), we are sure that the first key to expire in the set will exist for at least <code>MIN_VALIDITY=TTL-(T2-T1)-CLOCK_DRIFT</code>. All the other keys will expire later, so we are sure that the keys will be simultaneously set for at least this time.</p>

<p>During the time that the majority of keys are set, another client will not be able to acquire the lock, since <code>N/2+1</code> <code>SET NX</code> operations can’t succeed if <code>N/2+1</code> keys already exist. So if a lock was acquired, it is not possible to re-acquire it at the same time (violating the mutual exclusion property).</p>

<p>However we want to also make sure that multiple clients trying to acquire the lock at the same time can’t simultaneously succeed.</p>

<p>If a client locked the majority of instances using a time near, or greater, than the lock maximum validity time (the TTL we use for <code>SET</code> basically), it will consider the lock invalid and will unlock the instances, so we only need to consider the case where a client was able to lock the majority of instances in a time which is less than the validity time. In this case for the argument already expressed above, for <code>MIN_VALIDITY</code> no client should be able to re-acquire the lock. So multiple clients will be able to lock <code>N/2+1</code> instances at the same time (with &ldquo;time&rdquo; being the end of Step 2) only when the time to lock the majority was greater than the TTL time, making the lock invalid.</p>

<p>The system liveness is based on three main features:</p>

<ol>
<li>The auto release of the lock (since keys expire): eventually keys are available again to be locked.</li>
<li>The fact that clients, usually, will cooperate removing the locks when the lock was not acquired, or when the lock was acquired and the work terminated, making it likely that we don’t have to wait for keys to expire to re-acquire the lock.</li>
<li>The fact that when a client needs to retry a lock, it waits a time which is comparably greater than the time needed to acquire the majority of locks, in order to probabilistically make split brain conditions during resource contention unlikely.</li>
</ol>

<p>However, we pay an availability penalty equal to TTL time on network partitions, so if there are continuous partitions, we can pay this penalty indefinitely. This happens every time a client acquires a lock and gets partitioned away before being able to remove the lock.</p>

<p>Basically if there are infinite continuous network partitions, the system may become not available for an infinite amount of time.</p>

<h2 id="the-code">The code</h2>

<p>The sample code can be accessed on <a href="https://github.com/Wkalmar/LeaderElection">github</a>. Let&rsquo;s break down what actually happens here.</p>

<p>The idea behind leader election via distributed lock is whoever acquires lock over the shared resource becomes a leader. So naturally, we have a lock key quite similarly to built-in C# <code>lock</code> construct.</p>

<p><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-cs" data-lang="cs"><span style="color:#66d9ef">private</span> <span style="color:#66d9ef">const</span> <span style="color:#66d9ef">string</span> _resource = <span style="color:#e6db74">&#34;the-thing-we-are-locking-on&#34;</span>;
</code></pre></div>
Obviously, the storage is a single point of failure so we have to make sure that it is reliable. RedLock.net which we use for our case allows us to use multiple instances of Redis instead of single in order to improve reliability.</p>

<p>Here&rsquo;s how we create a connection to Redis during start-up.</p>

<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-cs" data-lang="cs"><span style="color:#66d9ef">var</span> endPoints = <span style="color:#66d9ef">new</span> List&lt;RedLockEndPoint&gt;
{
    <span style="color:#66d9ef">new</span> DnsEndPoint(<span style="color:#e6db74">&#34;redis1&#34;</span>, <span style="color:#ae81ff">6379</span>)
    <span style="color:#66d9ef">new</span> DnsEndPoint(<span style="color:#e6db74">&#34;redis2&#34;</span>, <span style="color:#ae81ff">6379</span>)
    <span style="color:#66d9ef">new</span> DnsEndPoint(<span style="color:#e6db74">&#34;redis3&#34;</span>, <span style="color:#ae81ff">6379</span>)
};
_distributedLockFactory = RedLockFactory.Create(endPoints);
</code></pre></div>

<p>In case you need to provide password you may take advantage of <code>RedLockEndPoint</code></p>

<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-cs" data-lang="cs"><span style="color:#66d9ef">var</span> endpoint = <span style="color:#66d9ef">new</span> RedLockEndPoint(<span style="color:#66d9ef">new</span> DnsEndPoint(<span style="color:#e6db74">&#34;localhost&#34;</span>, <span style="color:#ae81ff">49153</span>));
endpoint.Password = <span style="color:#e6db74">&#34;redispw&#34;</span>;
</code></pre></div>

<p>Each instance tries to acquire a lock once in a given period of time. If it succeeds it becomes a leader. If not, it will try once again later. <code>CreateLockAsync</code> has an overload that accepts the retry interval as well as the interval by which the lock is acquired. The method will block until the lock is acquired or until wait timeout provided as the parameter will expire</p>

<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-cs" data-lang="cs"><span style="color:#66d9ef">private</span> <span style="color:#66d9ef">async</span> Task TryAcquireLock(CancellationToken token)
{
    <span style="color:#66d9ef">if</span> (token.IsCancellationRequested)
        <span style="color:#66d9ef">return</span>;

    <span style="color:#66d9ef">var</span> distributedLock = <span style="color:#66d9ef">await</span> _distributedLockFactory.CreateLockAsync(
        _resource,
        _expiry,
        _wait,
        _retry,
        token);
    <span style="color:#66d9ef">if</span> (distributedLock.IsAcquired)
    {
        DoLeaderJob();
    }
}
</code></pre></div>

<p>As you can see the lock expires after the provided amount of time which is the part of auto release mechanism of RedLock algorithm. As an author of the algorithm notes</p>

<blockquote>
<p>A distributed lock without an auto release mechanism, where the lock owner will hold it indefinitely, is basically useless. If the client holding the lock crashes and does not recover with full state in a short amount of time, a deadlock is created where the shared resource that the distributed lock tried to protect remains forever unaccessible. This creates a liveness issue that is unacceptable in most situations, so a sane distributed lock must be able to auto release itself.</p>
</blockquote>

<p>However, you do not need to re-acquire the lock explicitly in your code since it has auto-extend feature. At first encounter with RedLock.net this might be unintuitive so it should be noted. In simple words, you might think of it as a sort of leader health-check built into RedLock algorithm.</p>

<p>As mentioned above we get rid of re-acquire timer as soon as an instance becomes a leader taking advantage of auto-extend feature.
Once the instance fails the lock is released and is up to other instances for grabs.</p>

<h2 id="summary">Summary</h2>

<p>As we can see the implementation of leader election via distributed lock is pretty straightforward. Still, it should be used with care since every locking increases contention between instances of a microservice and thus reduces the benefits of horizontal scaling.</p>

    </div>
    <div class="post-footer">
      
    </div>
  </article>

    </main>
  </body>
</html>
